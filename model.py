import torch
import torch.nn as nn
import torch.nn.functional as F

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) í…ìŠ¤íŠ¸ìš©: MLPProjector (512 â†’ 384 â†’ 256)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class MLPProjector(nn.Module):
    def __init__(self, input_dim=512, hidden_dims=(384,), output_dim=256):
        super().__init__()
        dims = [input_dim] + list(hidden_dims) + [output_dim]
        layers = []
        for i in range(len(dims) - 1):
            layers.append(nn.Linear(dims[i], dims[i + 1], bias=False))
            if i < len(dims) - 2:
                layers.extend([nn.BatchNorm1d(dims[i + 1]), nn.ReLU(inplace=True)])
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2) ì´ë¯¸ì§€ìš©: LinearProjector (512 â†’ 256)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class LinearProjector(nn.Linear):
    def __init__(self, input_dim=512, output_dim=256):
        super().__init__(input_dim, output_dim, bias=False)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3) Unmix-CLIP: ì „ì²´ ëª¨ë¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class UnmixCLIP(nn.Module):
    def __init__(self, clip_model, image_projector, text_projector):
        super().__init__()
        self.clip = clip_model.visual              # ResNet-101 Encoder
        self.clip_text = clip_model.encode_text    # Text Encoder
        self.image_projector = image_projector     # Linear: 512â†’256
        self.text_projector = text_projector       # MLP:    512â†’384â†’256

        for p in self.clip.parameters():
            p.requires_grad = False

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 3.1) CLIP ì´ë¯¸ì§€ ì¸ì½”ë”ì—ì„œ íŒ¨ì¹˜ ë²¡í„° (49ê°œ) ì¶”ì¶œ
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _forward_image_patches(self, images):
        """
        Input:  images [B, 3, 224, 224]
        Output: patch_embeds [B, 49, 512]
        """
        x = self.clip.conv1(images)
        x = self.clip.bn1(x)
        x = self.clip.relu1(x)
        x = self.clip.conv2(x)
        x = self.clip.bn2(x)
        x = self.clip.relu2(x)
        x = self.clip.conv3(x)
        x = self.clip.bn3(x)
        x = self.clip.relu3(x)

        x = self.clip.avgpool(x)
        x = self.clip.layer1(x)
        x = self.clip.layer2(x)
        x = self.clip.layer3(x)
        x = self.clip.layer4(x)           # [B, 2048, 7, 7]

        x = self.clip.attnpool(x)         # [B, 50, 512]  â†’ CLS + 49
        patch_tokens = x[:, 1:, :]        # [B, 49, 512]
        print("ğŸ‘‰ patch_tokens.shape: ", patch_tokens.shape) # ğŸ‘‰ patch_tokens.shape:  torch.Size([16, 196, 512])
        return patch_tokens

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 3.2) ëª¨ë¸ forward(images, text_tokens)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def forward(self, images, text_tokens):
        """
        images      : [B, 3, 224, 224]
        text_tokens : [2N, 77] (tokenized text prompts)
        Returns
        -------
        logits_pos, logits_neg : [B, N] Ã— 2
        txt_proj               : [2N, 256] (for MFI loss)
        """
        # â‘  ì´ë¯¸ì§€ ì¸ì½”ë”©
        with torch.no_grad():
            patches = self._forward_image_patches(images)  # [B, 49, 512]
        img_proj = self.image_projector(patches)           # [B, 49, 256]
        img_proj = F.normalize(img_proj, dim=-1)

        # â‘¡ í…ìŠ¤íŠ¸ ì¸ì½”ë”© (ê³ ì •)
        with torch.no_grad():
            txt_feat = self.clip_text(text_tokens)         # [2N, 512]
        txt_proj = self.text_projector(txt_feat)           # [2N, 256]
        txt_proj = F.normalize(txt_proj, dim=-1)

        # â‘¢ Conv1dë¡œ cosine ìœ ì‚¬ë„ ê³„ì‚°: [B, 2N, 49]
        O = 20 * F.conv1d(
            img_proj.transpose(1, 2),      # [B, 256, 49]
            txt_proj[:, :, None]           # [2N, 256, 1]
        )                                  # [B, 2N, 49]

        # â‘£ soft-weighted sum (DualCoOp ë°©ì‹)
        W = F.softmax(O, dim=-1)           # [B, 2N, 49]
        logits = 5 * (W * O).sum(-1)       # [B, 2N]

        # â‘¤ ì–‘/ìŒ ë¶„ë¦¬ (dual prompt)
        B, _2N = logits.size()
        N = _2N // 2
        logits = logits.view(B, 2, N)      # [B, 2, N]
        logits_pos = logits[:, 0]          # [B, N]
        logits_neg = logits[:, 1]          # [B, N]

        return logits_pos, logits_neg, txt_proj